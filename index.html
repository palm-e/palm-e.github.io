
<!DOCTYPE html>
<html>

<head lang="en">
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>PaLM-E: An Embodied Multimodal Language Model</title>

    <meta name="description" content="PaLM-E: An Embodied Multimodal Language Model">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- <base href="/"> -->

        <!--FACEBOOK-->
    <meta property="og:image" content="https://palm-e.github.io/img/palm-e-teaser.png">
    <meta property="og:image:type" content="image/png">
    <meta property="og:image:width" content="1262">
    <meta property="og:image:height" content="694">
    <meta property="og:type" content="website" />
    <meta property="og:url" content="https://palm-e.github.io/"/>
    <meta property="og:title" content="PaLM-E: An Embodied Multimodal Language Model" />
    <meta property="og:description" content="Project page for PaLM-E: An Embodied Multimodal Language Model." />

        <!--TWITTER-->
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="PaLM-E: An Embodied Multimodal Language Model" />
    <meta name="twitter:description" content="Project page for PaLM-E: An Embodied Multimodal Language Model." />
    <meta name="twitter:image" content="https://palm-e.github.io/img/palm-e-teaser.png" />


<!--     <link rel="apple-touch-icon" href="apple-touch-icon.png"> -->
  <!-- <link rel="icon" type="image/png" href="img/seal_icon.png"> -->
    <!-- Place favicon.ico in the root directory -->

    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-EVSTQN3/azprG1Anm3QDgpJLIm9Nao0Yz1ztcQTwFspd3yD65VohhpuuCOmLASjC" crossorigin="anonymous">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    <link rel="stylesheet" href="css/app.css">

    <link rel="stylesheet" href="css/bootstrap.min.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/js/bootstrap.bundle.min.js" integrity="sha384-MrcW6ZMFYlzcLA8Nl+NtUVF0sA7MsXsP1UyJoMp4YLEuNSfAP+JcXn/tWtIaxVXM" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>
    
    <script src="js/app.js"></script>
    <script>

function myFunction(imgs) {
    // Get the expanded image
    var expandImg = document.getElementById("expandedImg");
    // Get the image text
    var imgText = document.getElementById("imgtext");
    var answer = document.getElementById("answer");

    // Use the same src in the expanded image as the image being clicked on from the grid
    expandImg.src = imgs.src;
    // Use the value of the alt attribute of the clickable image as text inside the expanded image
    var qa = imgs.alt.split("[sep]");
    imgText.innerHTML = qa[0];
    answer.innerHTML = "";
    // Show the container element (hidden with CSS)
    expandImg.parentElement.style.display = "block";
    typeWriter(qa[1], 0, qa[0]);
    }

function typeWriter(txt, i, q) {
    var imgText = document.getElementById("imgtext");
    if (imgText.innerHTML == q) {
    if (i < txt.length) {
        document.getElementById("answer").innerHTML += txt.charAt(i);
        i++;
        setTimeout(typeWriter, 20, txt, i, q);
    }
    }
}
</script>
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-52J0PM8XKV"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-52J0PM8XKV');
</script>
	
    <style>
        .nav-pills {
          position: relative;
          display: inline;
        }
        .imtip {
          position: absolute;
          top: 0;
          left: 0;
        }
    </style>
</head>

<body>
    <div class="container" id="main">
        
        <div class="row">
            <p></p> <p></p>
            <div class="col-md-12 text-center">
                <strong><font size="+3">PaLM-E: An Embodied Multimodal Language Model</font></strong> 
            </div>
        </div>
        <div class="row">
            <div class="col-md-12 text-center">
                
                <ul class="list-inline">
                    <br>
                    <li>Danny Driess<sup>1,2</sup></li> <li>Fei Xia<sup>1</sup> </li> <li> Mehdi S. M. Sajjadi<sup>3</sup> </li> <li> Corey Lynch<sup>1</sup></li> <li>Aakanksha Chowdhery<sup>3</sup> </li>  <br>
                    <li> Brian Ichter<sup>1</sup></li> <li>  Ayzaan Wahid<sup>1</sup> </li> <li> Jonathan Tompson<sup>1</sup> </li> <li> Quan Vuong<sup>1</sup> </li> <li> Tianhe Yu<sup>1</sup> </li> <li> Wenlong Huang<sup>1</sup> </li> <br>
                    <li> Yevgen Chebotar<sup>1</sup> </li>  <li> Pierre Sermanet<sup>1</sup> </li> <li> Daniel Duckworth<sup>3</sup> </li> <li> Sergey Levine<sup>1</sup> </li> <li> Vincent Vanhoucke<sup>1</sup> </li>  <br>
                    <li> Karol Hausman<sup>1</sup> </li> <li> Marc Toussaint<sup>2</sup></li> <li> Klaus Greff<sup>3</sup> </li> <li> Andy Zeng<sup>1</sup> </li><li> Igor Mordatch<sup>3</sup> </li> <li> Pete Florence<sup>1</sup> </li>
                    <br><br>
                    <sup>1</sup><a href="http://g.co/robotics">
                        <img src="img/rng-logo.png" height="37px"> </a>
                        <sup>2</sup><a href="https://www.tu.berlin/en/">
                        <img src="img/tuberlin.png" height="32px"> </a> &nbsp;&nbsp;
                        <sup>3</sup><a href="https://research.google/teams/brain/">  
                        <img src="img/google-research-logo.png" height="25px"> </a> 
                        
                    </ul>

            </div>
        </div>


        <div class="row">
            <div class="col-md-4 col-md-offset-4 text-center">
                <ul class="nav nav-pills nav-justified">
                    <li>
                        <a href="#">
                        <img src="img/paper_thumb.png" height="60px">
                            <h4><strong>Paper</strong></h4>
                        </a>
                    </li>

                    <!-- <li>
                        <a href="#">
                        <img src="img/youtube_icon.png" height="60px">
                            <h4><strong>Video</strong></h4>
                        </a>
                    </li> -->
<!--                         <li>
                        <a href="http://ai.googleblog.com/2022/12/rt-1-robotics-transformer-for-real.html">
                        <image src="img/google-ai-blog-small.png" height="60px">
                            <h4><strong>Blogpost</strong></h4>
                        </a>
                    </li>
                     <li>
                        <a href="https://github.com/google-research/robotics_transformer">
                        <image src="img/github.png" height="60px">
                            <h4><strong>Code</strong></h4>
                        </a>                   
                    </li>  -->
                </ul>
            </div>
        </div>

        <div class="row">
            
            <div class="col-md-8 col-md-offset-2 col-xs-12">
                <video id="v0" width="100%" playsinline muted loop autoplay onclick="setAttribute('controls', 'true');">
                    <source src="videos/palm-e-teaser.mp4" type="video/mp4">
                </video>		

                <h3>
                    Abstract
                </h3>
                <p class="text-justify">

                    Large language models have been demonstrated to perform complex tasks. However, enabling general inference in the real world, e.g. for robotics problems, raises the challenge of grounding. We propose embodied language models to directly incorporate real-world continuous sensor modalities into language models and thereby establish the link between words and percepts. Input to our embodied language model are multi-modal sentences that interleave visual, continuous state estimation, and textual input encodings. We train these encodings end-to-end, in conjunction with a pre-trained large language model, for multiple embodied tasks, including sequential robotic manipulation planning, visual question answering, and captioning.
                    
                    Our evaluations show that PaLM-E, a single large embodied multimodal model, can address a variety of embodied reasoning tasks, from a variety of observation modalities, on multiple embodiments, and further, exhibits <i>positive transfer</i>: the model benefits from diverse joint training across internet-scale language, vision, and visual-language domains.
                    Our largest model, PaLM-E-562B with 562B parameters, in addition to being trained on robotics tasks, is a visual-language generalist with state-of-the-art performance on OK-VQA, and retains generalist language capabilities with increasing scale.
                </p>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2 col-xs-12">
                <h3>
                    Approach
                </h3>
                <p style="text-align:center;">
                    <img src="img/approach.png" class="img-responsive">
                </p>

                <p class="text-justify">
                    The main architectural idea of PaLM-E is to inject continuous, embodied observations such as images, 
                    state estimates, or other sensor modalities into the language embedding space of a pre-trained language model. 
                    This is realized by encoding the continuous observations into a sequence of vectors with the same dimension 
                    as the embedding space of the language tokens. The continuous information is hence injected into the language model 
                    in an analogous way to language tokens. PaLM-E is a decoder-only LLM that generates textual completions 
                    autoregressively given a prefix or prompt. We call our model PaLM-<b>E</b>, since we use <a href="https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html">PaLM (Chowdhery et al., 2022)</a> as 
                    the pre-trained language model, and make it <b>E</b>mbodied.
                </p>    
            </div>
        </div>




    <div class="row">
            <div class="col-md-8 col-md-offset-2 col-xs-12">
                <h3>
                    Results
                </h3>
		<p class="text-justify">
        <p style="text-align:center;">
            <video id="v0" width="100%" playsinline autoplay muted loop controls>
                <source src="videos/meta/planning.mp4" type="video/mp4">
            </video>
        </p>
        
		We show a few example videos showing how PaLM-E can be used to plan and execute long horizon tasks on two different real embodiments. Please note, that all of these results were obtained using the same model trained on all data.
		In the first video, we execute a long-horizon instruction "bring me the rice chips from the drawer" that includes multiple planning steps as well as incorporating visual feedback from the robot's camera. <br><br>
		Below, we show another example on the same robot where the instruction is "bring me a green star". Green star is an object that this robot wasn't directly exposed to.
		</p>
        

        <p style="test-align:center;">
					<video id="v0" width="100%" playsinline muted loop controls>
                       <source src="videos/meta/green_star.mp4" type="video/mp4">
                   </video>		
        </p>
        <p class="text-justify">

         In the next example, we show the exact same PaLM-E model weights on another robot embodiment. In this case our model is able to successfully plan a long-horizon task "sort blocks by colors into different corners" .
         Here is another example of this tasks that involves planning over multiple stages and incorporating visual feedback over long time horizons.
         Below, we demonstate another example of long-horizon pushing tasks on this robot. The first instruction is "move remaining blocks to the group".  PaLM-E sequences step-by-step commands to the low-level policy such as "move the yellow hexagon to the green start", and "move the blue triangle to the group".
         Whereas in this video the instruction is: "push the ocean colored blocks together".
        </p>

        <div id="carouselExampleCaptions" class="carousel slide carousel-dark carousel-fade" data-bs-ride="carousel">
            <div class="carousel-indicators">
              <button type="button" data-bs-target="#carouselExampleCaptions" data-bs-slide-to="0" class="active" aria-current="true" aria-label="Slide 1"></button>
              <button type="button" data-bs-target="#carouselExampleCaptions" data-bs-slide-to="1" aria-label="Slide 2"></button>
              <button type="button" data-bs-target="#carouselExampleCaptions" data-bs-slide-to="2" aria-label="Slide 3"></button>
              <button type="button" data-bs-target="#carouselExampleCaptions" data-bs-slide-to="3" aria-label="Slide 4"></button>
            </div>
            <div class="carousel-inner">
              <div class="carousel-item active" data-bs-interval="100000000000">
                <video id="v0" width="100%" playsinline muted loop controls autoplay>
                    <source src="videos/xarm/sort_by_color_iphone.mp4" type="video/mp4">
                </video>		
                <div class="carousel-caption d-none d-md-block">
                  <h5>sort blocks by colors into different corners</h5>
                  <!-- <p>Some representative placeholder content for the first slide.</p> -->
                </div>
              </div>
              <div class="carousel-item" data-bs-interval="100000000000">
                <video id="v0" width="100%" playsinline muted loop controls autoplay>
                    <source src="videos/xarm/sort_colors_into_corners_long.mp4" type="video/mp4">
                </video>	
                <div class="carousel-caption d-none d-md-block">
                  <h5>Incorporating visual feedback over long time horizons</h5>
                  <!-- <p>Some representative placeholder content for the second slide.</p> -->
                </div>
              </div>
              <div class="carousel-item" data-bs-interval="100000000000">
                <video id="v0" width="100%" playsinline muted loop controls autoplay>
                    <source src="videos/xarm/remaining_blocks_to_group.mp4" type="video/mp4">
                </video>		
                <div class="carousel-caption d-none d-md-block">
                  <h5>Move remaining blocks to the group</h5>
                  <!-- <p>Some representative placeholder content for the third slide.</p> -->
                </div>
              </div>
              <div class="carousel-item" data-bs-interval="100000000000">
                <video id="v0" width="100%" playsinline muted loop controls autoplay>
                    <source src="videos/xarm/ocean_colored_blocks_together.mp4" type="video/mp4">
                </video>		
                <div class="carousel-caption d-none d-md-block">
                  <h5>Push the ocean colored blocks together</h5>
                  <!-- <p>Some representative placeholder content for the third slide.</p> -->
                </div>
              </div>
            </div>
            <button class="carousel-control-prev" type="button" style="max-height:21%; margin-top:21%;" data-bs-target="#carouselExampleCaptions" data-bs-slide="prev">
              <span class="carousel-control-prev-icon" aria-hidden="true"></span>
              <span class="visually-hidden">Previous</span>
            </button>
            <button class="carousel-control-next" type="button" style="max-height:21%; margin-top:21%;" data-bs-target="#carouselExampleCaptions" data-bs-slide="next">
              <span class="carousel-control-next-icon" aria-hidden="true"></span>
              <span class="visually-hidden">Next</span>
            </button>
          </div>

        <!-- <p style="test-align:center;">
					<video id="v0" width="100%" playsinline muted loop controls>
                       <source src="videos/xarm/sort_by_color_iphone.mp4" type="video/mp4">
                   </video>		
        </p>
        
		</p>
        <p style="test-align:center;">
					<video id="v0" width="100%" playsinline muted loop controls>
                       <source src="videos/xarm/sort_colors_into_corners_long.mp4" type="video/mp4">
                   </video>		
        </p>
         -->
		<!-- </p>
        <p style="test-align:center;">
					<video id="v0" width="100%" playsinline muted loop controls>
                       <source src="videos/xarm/remaining_blocks_to_group.mp4" type="video/mp4">
                   </video>		
        </p> -->
		<!-- </p>
        <p style="test-align:center;">
					<video id="v0" width="100%" playsinline muted loop controls>
                       <source src="videos/xarm/ocean_colored_blocks_together.mp4" type="video/mp4">
                   </video>		
        </p> -->
        
        <p>
        Next, we demonstrate two examples of generalization. In the case below the instruction is "push red blocks to the coffee cup". The dataset contains only three demonstrations with the coffee cup in them, and none of them included red blocks.
		</p>
 
        <p>
        Lastly, we show another generalization example, where the instruction is "push green blocks to the turtle". The robot is able to successfully execute this task even though it has never seen the turtle before.
		</p>

        <div id="carouselExampleCaptions2" class="carousel slide carousel-dark carousel-fade" data-bs-ride="carousel">
            <div class="carousel-indicators">
              <button type="button" data-bs-target="#carouselExampleCaptions2" data-bs-slide-to="0" class="active" aria-current="true" aria-label="Slide 1"></button>
              <button type="button" data-bs-target="#carouselExampleCaptions2" data-bs-slide-to="1" aria-label="Slide 2"></button>
            </div>
            <div class="carousel-inner">
              <div class="carousel-item active" data-bs-interval="100000000000">
                <video id="v0" width="100%" playsinline muted loop controls autoplay>
                    <source src="videos/xarm/red_blocks_to_coffee.mp4" type="video/mp4">
                </video>		
                <div class="carousel-caption d-none d-md-block">
                  <h5>Push red blocks to the coffee cup</h5>
                  <!-- <p>Some representative placeholder content for the first slide.</p> -->
                </div>
              </div>
              <div class="carousel-item" data-bs-interval="100000000000">
                <video id="v0" width="100%" playsinline muted loop controls autoplay>
                    <source src="videos/xarm/green_blocks_to_turtle.mp4" type="video/mp4">
                </video>	
                <div class="carousel-caption d-none d-md-block">
                  <h5>Push green blocks to the turtle</h5>
                  <!-- <p>Some representative placeholder content for the second slide.</p> -->
                </div>
              </div>
            </div>
            <button class="carousel-control-prev" type="button" style="max-height:21%; margin-top:21%;" data-bs-target="#carouselExampleCaptions2" data-bs-slide="prev">
              <span class="carousel-control-prev-icon" aria-hidden="true"></span>
              <span class="visually-hidden">Previous</span>
            </button>
            <button class="carousel-control-next" type="button" style="max-height:21%; margin-top:21%;" data-bs-target="#carouselExampleCaptions2" data-bs-slide="next">
              <span class="carousel-control-next-icon" aria-hidden="true"></span>
              <span class="visually-hidden">Next</span>
            </button>
          </div>

	    </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2 col-xs-12">
                <h3>
                    Demo
                </h3>
                <div class="row">
                    <div class="col-md-2 col-sm-2 col-xs-4">
                      <img src="img/demo_img/bike.png" width="100%" alt="Given <b>img</b>. Q: Can I go down this street on a bicycle, yes or no? A: Let's think step by step. [sep] 1. do not enter. 2. except bicycles. 3. do not enter except bicycles. 4. yes." onclick="myFunction(this);">
                    </div>
                    <div class="col-md-2 col-sm-2 col-xs-4">
                      <img src="img/demo_img/donut.png" width="100%" alt="Given <b>img</b>. Q: What flavor is the pink doughnut on the right? A:[sep] strawberry.. Q: What flavor is the doughnut on the left? A blueberry. " onclick="myFunction(this);">
                    </div>
                    <div class="col-md-2 col-sm-2 col-xs-4">
                      <img src="img/demo_img/darpa-robot.jpg" width="100%" alt="Given <b>img</b>. Q: What will the robot do next? A: [sep] fall." onclick="myFunction(this);">
                    </div>
                    <div class="col-md-2 col-sm-2 col-xs-4">
                      <img src="img/demo_img/cake.jpg" width="100%" alt="Robot: I am a robot operating in a kitchen. Given <b>img</b>, When a human asks me to do a task, I will respond with the sequence of actions I would do to accomplish the task with only the items I see.  Human: Use all of the ingredients you see to make a cake batter. [sep]1. crack egg. 2. put egg in bowl. 3. put flour in bowl. 4. put sugar in bowl. 5. mix. 6. put in pan. 7. bake. 8. eat. 9. clean up." onclick="myFunction(this);">
                    </div>
                    <div class="col-md-2 col-sm-2 col-xs-4">
                      <img src="img/demo_img/compare.png" width="100%" alt="Photo 1: <b>img1</b>.  Photo 2: <b>img2</b>.  Q: What is in Photo 1 but NOT in Photo 2? A: Let's think step by step and explain.[sep] Photo 1 has sunglasses on top of folded clothes. Photo 2 does not have sunglasses on top of folded clothes. Therefore, sunglasses are in Photo 1 but not in Photo 2." onclick="myFunction(this);">
                    </div>
                    <div class="col-md-2 col-sm-2 col-xs-4">
                      <img src="img/demo_img/donkey.jpg" width="100%" alt="I see (image omitted). Description: A cow sitting on a rug. Joke: Why is the cow sitting on a rug? Because it wants to be a cow-ch!I see <b>img</b>.[sep] Description: a donkey is carrying a dog, cat, and rooster. Joke: what do you call a donkey with a rooster on his back. A rooster booster. "
                       onclick="myFunction(this);">
                    </div>
                    <p></p>

                    <div class="col-md-2 col-sm-2 col-xs-4">
                        <img src="img/demo_img/kobe.jpg" width="100%" alt="Given <b>img</b>. Q: How many championship rings did the player on the left win in his career A: Letâ€™s think step by step. [sep] 1. kobe bryant is on the left. 2. kobe bryant has won 5 championship rings." onclick="myFunction(this);">
                    </div>
                    <div class="col-md-2 col-sm-2 col-xs-4">
                        <img src="img/demo_img/knicks_celtics.jpg" width="100%" alt="Given <b>img</b>. Q: Who are the two teams playing in this photo, and which was the last to win a championship, and which year did they win, and who was their star player that year? A: Let's think step by step. [sep]  1. team in white is new york knick. 2. team in green is boston celtic. 3. last to win championship was boston celtic. 4. year was 2008. 5. star player was paul pierce. 6. i am not sure if this is correct. i am not basketball fan. i just google it. " onclick="myFunction(this);">
                    </div>
                    <div class="col-md-2 col-sm-2 col-xs-4">
                        <img src="img/demo_img/fruits.jpg" width="100%" alt="Given <b>img</b>. Q: What's in the image? Answer in emojis. A: [sep]ðŸðŸŒðŸ‡ðŸðŸ‘ðŸˆðŸ’. " onclick="myFunction(this);">
                    </div>
                    <div class="col-md-2 col-sm-2 col-xs-4">
                        <img src="img/demo_img/menu2.jpg" width="100%" alt="Given <b>img</b>. Q: I am getting just two custom pizza for me and my friend. How much should I play in total? A: Let's think step by step. [sep] 1st step: 2 custom pizzas. 2nd step: 2 x $9.99. 3rd step: $19.98." onclick="myFunction(this);">
                    </div>
                    <div class="col-md-2 col-sm-2 col-xs-4">
                        <img src="img/demo_img/cleaning.jpg" width="100%" alt="Given <b>img</b>. Q: If a robot wanted to be useful here, what steps should it take? A: [sep]clean table..pick up trash.. pick up chairs.. wipe chairs.. put chairs down." onclick="myFunction(this);">
                    </div>
                    <div class="col-md-2 col-sm-2 col-xs-4">
                        <img src="img/demo_img/diary.png" width="100%" alt=" Photo 1, at 10:30 am: <b>img1</b>. Photo 2, at 12:45 pm: <b>img2</b>. Photo 3, at 3:45 pm: <b>img3</b>. Q: I forget, what did I have for lunch, and what time was it? A: Let's think step by step. [sep]1. you had a sandwich for lunch. 2. it was 12:45 pm." onclick="myFunction(this);">
                    </div>
                </div>
                <p></p>

                <div class="row">
                    <div class="col-md-6">
                        <img class="img img-responsive" id="expandedImg" style="height: 300px;" src="img/placeholder.png">
                    </div>
                    <div class="col-md-6">
                    <div id="imgtext">Prompt text in gray.</div>
                    <div><p style="test-align:center" id="answer">PaLM-E response in orange shade.</p></div>
                    </div>
                    
                </div>
        
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2 col-xs-12">
                <h3>
                    Citation 
                </h3>
                <div class="form-group col-md-10 col-md-offset-1">
                    <textarea id="bibtex" class="form-control" readonly>
@inproceedings{driess2023palme,
    title={PaLM-E: An Embodied Multimodal Language Model},
    author={},
    booktitle={arXiv preprint arXiv:2302.11111},
    year={2023}
}</textarea>
                </div>
            </div>
        </div>


        <div class="row">
            <div class="col-md-8 col-md-offset-2 col-xs-12">
                <h3>
                    Acknowledgements
                </h3>
                <p class="text-justify">
                    The authors would like to thank, for their advice, help
                    and support: Xi Chen, Etienne Pot, Sebastian Goodman,
                    Ted Xiao, Keerthana Gopalakrishnan, Kehang Han, Henryk
                    Michalewski, Neil Houlsby, Basil Mustafa, Justin Gilmer,
                    Yonghui Wu, Erica Moreira, Victor Gomes, Tom Duerig,
                    and Kendra Byrne.
                </p>
            </div>
        </div>





    </div>
</body>
</html>
